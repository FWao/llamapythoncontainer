services:
  llama_cpp:
    build:
      context: .  # Directory containing the Dockerfile
    image: ghcr.io/fwao/llamapythoncontainer:latest
    container_name: llamapythoncontainer
    volumes:
      - ./models:/workspace/models  # Map host's ./models directory to /workspace/models in the container
      - ./data:/workspace/data  # Put main.py and additional data (e.g. input and output files) here and tell the python script where to find them
    working_dir: /workspace
    command: python /workspace/data/main.py --model /workspace/models/Llama-3.2-3B-Instruct-Q8_0.gguf
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
